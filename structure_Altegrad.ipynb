{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdq_zhclcKEY"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zu6Dh5lN261"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avaN9SNwQNU0"
      },
      "outputs": [],
      "source": [
        "pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cu116.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDkMucYsHNQ3"
      },
      "source": [
        " # Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOQEyyDsg1bd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "from torch_geometric.nn import GINConv,GINEConv,PDNConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG-X20nVcPAJ"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB5P6bp3h0bz"
      },
      "outputs": [],
      "source": [
        "PATH = \"drive/MyDrive/Altegra_data\"\n",
        "\n",
        "def load_data(new_edge_loading=False): \n",
        "    \"\"\"\n",
        "    Function that loads graphs\n",
        "    \"\"\"  \n",
        "    graph_indicator = np.loadtxt(os.path.join(PATH,\"graph_indicator.txt\"), dtype=np.int64)\n",
        "    _,graph_size = np.unique(graph_indicator, return_counts=True)\n",
        "    \n",
        "    edges = np.loadtxt(os.path.join(PATH,\"edgelist.txt\"), dtype=np.int64, delimiter=\",\")\n",
        "    edges_inv = np.vstack((edges[:,1], edges[:,0]))\n",
        "    edges = np.vstack((edges, edges_inv.T))\n",
        "    s = edges[:,0]*graph_indicator.size + edges[:,1]\n",
        "    idx_sort = np.argsort(s)\n",
        "    edges = edges[idx_sort,:]\n",
        "    edges,idx_unique =  np.unique(edges, axis=0, return_index=True)\n",
        "    A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n",
        "    \n",
        "    \n",
        "    x = np.loadtxt(os.path.join(PATH,\"node_attributes.txt\"), delimiter=\",\")\n",
        "    edge_attr = np.loadtxt(os.path.join(PATH,\"edge_attributes.txt\"), delimiter=\",\")\n",
        "    edge_attr = np.vstack((edge_attr,edge_attr))\n",
        "    edge_attr = edge_attr[idx_sort,:]\n",
        "    edge_attr = edge_attr[idx_unique,:]\n",
        "    \n",
        "    adj = []\n",
        "    features = []\n",
        "    edge_features = []\n",
        "    idx_n = 0\n",
        "    idx_m = 0\n",
        "    for i in range(graph_size.size):\n",
        "        adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n",
        "        edge_features.append(edge_attr[idx_m:idx_m+adj[i].nnz,:])\n",
        "        features.append(x[idx_n:idx_n+graph_size[i],:])\n",
        "        idx_n += graph_size[i]\n",
        "        idx_m += adj[i].nnz\n",
        "\n",
        "    return adj, features, edge_features\n",
        "\n",
        "def normalize_adjacency(A):\n",
        "    \"\"\"\n",
        "    Function that normalizes an adjacency matrix\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    A += sp.identity(n)\n",
        "    degs = A.dot(np.ones(n))\n",
        "    inv_degs = np.power(degs, -1)\n",
        "    D = sp.diags(inv_degs)\n",
        "    A_normalized = D.dot(A)\n",
        "\n",
        "    return A_normalized\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"\n",
        "    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
        "    \"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCONwNCocSI_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-kYd--wi0JK"
      },
      "outputs": [],
      "source": [
        "# from https://mlabonne.github.io/blog/gin/\n",
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"GIN\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.lin1 = Linear(hidden_dim*3, hidden_dim*3)\n",
        "        self.lin2 = Linear(hidden_dim*3, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node embeddings \n",
        "        h1 = self.conv1(x, edge_index)\n",
        "        h2 = self.conv2(h1, edge_index)\n",
        "        h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = self.dropout(h)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maTikjVGjhXy"
      },
      "outputs": [],
      "source": [
        "class GINE(torch.nn.Module):\n",
        "    \"\"\"GINE\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GINE, self).__init__()\n",
        "        self.conv1 = GINEConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        self.conv2 = GINEConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        self.conv3 = GINEConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        \n",
        "        self.conv4 = GINEConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        self.conv5 = GINEConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        self.conv6 = GINEConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()),edge_dim=5)\n",
        "        self.lin1 = Linear(hidden_dim*3, hidden_dim*3)\n",
        "        self.lin2 = Linear(hidden_dim*3, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_features, batch):\n",
        "\n",
        "        h1 = self.conv1(x, edge_index, edge_features)\n",
        "        h2 = self.conv2(h1, edge_index, edge_features)\n",
        "        h3 = self.conv3(h2, edge_index, edge_features)\n",
        "\n",
        "        # # Node embeddings \n",
        "        h1 = self.conv4(h1, edge_index, edge_features)\n",
        "        h2 = self.conv5(h2, edge_index, edge_features)\n",
        "        h3 = self.conv6(h3, edge_index, edge_features)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = self.dropout(h)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTO6Vs2UVRun"
      },
      "outputs": [],
      "source": [
        "# from https://mlabonne.github.io/blog/gin/\n",
        "class GIN2Graphs(torch.nn.Module):\n",
        "    \"\"\"GIN2Graphs\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GIN2Graphs, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "\n",
        "        self.conv4 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv5 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv6 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "        self.lin1 = Linear(hidden_dim*6, hidden_dim*6)\n",
        "        self.lin2 = Linear(hidden_dim*6, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index_distance_based,edge_index_bond_based, batch):\n",
        "        # First Graph info\n",
        "        h1 = self.conv1(x, edge_index_distance_based)\n",
        "        h2 = self.conv2(h1, edge_index_distance_based)\n",
        "        h3 = self.conv3(h2, edge_index_distance_based)\n",
        "\n",
        "        h4 = self.conv4(x, edge_index_bond_based)\n",
        "        h5 = self.conv5(h4, edge_index_bond_based)\n",
        "        h6 = self.conv6(h5, edge_index_bond_based)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h4 = global_add_pool(h4, batch)\n",
        "        h5 = global_add_pool(h5, batch)\n",
        "        h6 = global_add_pool(h6, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3,h4,h5,h6), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = self.dropout(h)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKTEbYfFeOys"
      },
      "outputs": [],
      "source": [
        "class PDN(torch.nn.Module):\n",
        "    \"\"\" Pathfinder Discovery Networks for Neural Message Passing paper\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super().__init__()\n",
        "        self.conv1 = PDNConv(input_dim, hidden_dim, edge_dim=5, hidden_channels=2)\n",
        "        self.conv2 = PDNConv(hidden_dim, hidden_dim, edge_dim=5, hidden_channels=2)\n",
        "        self.conv3 = PDNConv(hidden_dim, hidden_dim, edge_dim=5, hidden_channels=2)\n",
        "        self.lin1 = Linear(hidden_dim, hidden_dim)\n",
        "        self.lin2 = Linear(hidden_dim, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_features, batch):\n",
        "        # Node embeddings \n",
        "        h1 = self.conv1(x, edge_index, edge_features)\n",
        "        h2 = self.conv2(h1, edge_index, edge_features)\n",
        "        h3 = self.conv3(h2, edge_index, edge_features)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h = global_add_pool(h3, batch)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = self.dropout(h)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEQZsgtvjHpa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "class SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "        self.act = torch.nn.ReLU()\n",
        "        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
        "        self.update_act = torch.nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        \n",
        "        \n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        \n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "\n",
        "    def message(self, x_j):\n",
        "        # x_j has shape [E, in_channels]\n",
        "\n",
        "        x_j = self.lin(x_j)\n",
        "        x_j = self.act(x_j)\n",
        "        \n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "\n",
        "\n",
        "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
        "        \n",
        "        new_embedding = self.update_lin(new_embedding)\n",
        "        new_embedding = self.update_act(new_embedding)\n",
        "        \n",
        "        return new_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whNu6t_gi9iJ"
      },
      "outputs": [],
      "source": [
        "embed_dim = 16\n",
        "from torch_geometric.nn import TopKPooling\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch.nn.functional as F\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self,input_dim,num_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv0 = GINConv(\n",
        "            Sequential(Linear(input_dim, embed_dim),\n",
        "                       BatchNorm1d(embed_dim), ReLU(),\n",
        "                       Linear(embed_dim, embed_dim), ReLU()))\n",
        "        self.conv1 = SAGEConv(embed_dim, embed_dim)\n",
        "        self.pool1 = TopKPooling(embed_dim, ratio=0.8)\n",
        "        self.conv2 = SAGEConv(embed_dim, embed_dim)\n",
        "        self.pool2 = TopKPooling(embed_dim, ratio=0.8)\n",
        "        self.conv3 = SAGEConv(embed_dim, embed_dim)\n",
        "        self.pool3 = TopKPooling(embed_dim, ratio=0.8)\n",
        "        # self.item_embedding = torch.nn.Embedding(num_embeddings=df.item_id.max() +1, embedding_dim=embed_dim)\n",
        "        self.lin1 = torch.nn.Linear(embed_dim*2, embed_dim)\n",
        "        self.lin2 = torch.nn.Linear(embed_dim, embed_dim//2)\n",
        "        self.lin3 = torch.nn.Linear(embed_dim//2, num_classes)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(embed_dim)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(embed_dim//2)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.act2 = torch.nn.ReLU()        \n",
        "  \n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # x = self.item_embedding(x)\n",
        "        # x = x.squeeze(1)     \n",
        "        x = F.relu(self.conv0(x, edge_index))\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        tmp = self.pool1(x, edge_index, None, batch)\n",
        "        x, edge_index, _, batch, _, _ = tmp\n",
        "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "     \n",
        "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
        "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "\n",
        "        x, edge_index, _, batch, _, _  = self.pool3(x, edge_index, None, batch)\n",
        "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = x1 + x2 + x3\n",
        "\n",
        "        x = self.lin1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.act2(x)      \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = F.log_softmax(self.lin3(x), dim=1).squeeze(1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://mlabonne.github.io/blog/gin/\n",
        "class GIN4Graphs(torch.nn.Module):\n",
        "    \"\"\"GIN4Graphs\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GIN4Graphs, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "\n",
        "        self.conv4 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv5 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv6 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "        self.conv7 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv8 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv9 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "        self.conv10 = GINConv(\n",
        "            Sequential(Linear(input_dim, hidden_dim),\n",
        "                       BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv11 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        self.conv12 = GINConv(\n",
        "            Sequential(Linear(hidden_dim, hidden_dim), BatchNorm1d(hidden_dim), ReLU(),\n",
        "                       Linear(hidden_dim, hidden_dim), ReLU()))\n",
        "        \n",
        "        self.lin1 = Linear(hidden_dim*3 * 4, hidden_dim*3 * 4)\n",
        "        self.lin2 = Linear(hidden_dim*3 * 4, n_class)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index_type1,edge_index_type2,edge_index_type3,edge_index_type4, batch):\n",
        "        # First Graph info\n",
        "        h1 = self.conv1(x, edge_index_type1)\n",
        "        h2 = self.conv2(h1, edge_index_type1)\n",
        "        h3 = self.conv3(h2, edge_index_type1)\n",
        "\n",
        "        h4 = self.conv4(x, edge_index_type2)\n",
        "        h5 = self.conv5(h4, edge_index_type2)\n",
        "        h6 = self.conv6(h5, edge_index_type2)\n",
        "\n",
        "        h7 = self.conv7(x, edge_index_type3)\n",
        "        h8 = self.conv8(h7, edge_index_type3)\n",
        "        h9 = self.conv9(h8, edge_index_type3)\n",
        "\n",
        "        h10 = self.conv10(x, edge_index_type4)\n",
        "        h11 = self.conv11(h10, edge_index_type4)\n",
        "        h12 = self.conv12(h11, edge_index_type4)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h4 = global_add_pool(h4, batch)\n",
        "        h5 = global_add_pool(h5, batch)\n",
        "        h6 = global_add_pool(h6, batch)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h7 = global_add_pool(h7, batch)\n",
        "        h8 = global_add_pool(h8, batch)\n",
        "        h9 = global_add_pool(h9, batch)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h10 = global_add_pool(h10, batch)\n",
        "        h11 = global_add_pool(h11, batch)\n",
        "        h12 = global_add_pool(h12, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3,h4,h5,h6,h7, h8, h9,h10,h11,h12), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = self.dropout(h)\n",
        "        h = self.lin2(h)\n",
        "        \n",
        "        return h, F.log_softmax(h, dim=1)"
      ],
      "metadata": {
        "id": "WYLe4VROEhKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6E7MILcoi-"
      },
      "source": [
        "# Getting Data and Choosing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KirejCvUkqWV"
      },
      "outputs": [],
      "source": [
        "adj, features, edge_features = load_data() \n",
        "# Normalize adjacency matrices\n",
        "adj = [normalize_adjacency(A) for A in adj]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbymrMs0i5wH"
      },
      "outputs": [],
      "source": [
        "adj_train_no_split = list()\n",
        "features_train_no_split = list()\n",
        "edges_features_train_no_split = list()\n",
        "y_train_no_split = list()\n",
        "adj_test = list()\n",
        "features_test = list()\n",
        "edges_features_test = list()\n",
        "proteins_test = list()\n",
        "print(\"Loading Data\")\n",
        "\n",
        "method_number = 2\n",
        "#normalizing values\n",
        "if method_number == 1:\n",
        "  #Method 1\n",
        "  MEAN_COORDS = np.zeros(3)\n",
        "  MIN_COORDS = np.zeros(3)\n",
        "  MAX_COORDS = np.zeros(3)\n",
        "#Method 2\n",
        "elif method_number == 2:\n",
        "  from sklearn.preprocessing import RobustScaler\n",
        "  transformer = RobustScaler()\n",
        "  transformer_amino = RobustScaler()\n",
        "  transformer_dist = RobustScaler()\n",
        "  all_coords = []\n",
        "  all_features = []\n",
        "  all_dist = []\n",
        "# IMPORTANT: Only normalize coordinates (0-2) and amino acid features (26-86)\n",
        "with open(os.path.join(PATH,'graph_labels.txt'), 'r') as f:\n",
        "    for i,line in tqdm(enumerate(f)):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) == 0:\n",
        "            proteins_test.append(t[0])\n",
        "            adj_test.append(adj[i])\n",
        "            features_test.append(features[i])\n",
        "            edges_features_test.append(edge_features[i])\n",
        "        else:\n",
        "            adj_train_no_split.append(adj[i])\n",
        "            feat = features[i]\n",
        "            efeat = edge_features[i]\n",
        "            #Method 1\n",
        "            if method_number == 1:\n",
        "              MEAN_COORDS += np.mean(feat[:,0:3],axis=0)\n",
        "              MIN_COORDS = np.min([MIN_COORDS,np.min(feat[:,0:3],axis=0)],axis=0)\n",
        "              MAX_COORDS = np.max([MAX_COORDS,np.max(feat[:,0:3],axis=0)],axis=0)\n",
        "            #Method 2\n",
        "            elif method_number == 2:\n",
        "              all_coords.append(feat[:,0:3])\n",
        "              all_features.append(feat[:,25:])\n",
        "              all_dist.append(efeat[:,0])\n",
        "\n",
        "            # feat = normalize_features(feat)\n",
        "            features_train_no_split.append(feat)\n",
        "            edges_features_train_no_split.append(efeat)\n",
        "            y_train_no_split.append(int(t[1][:-1]))\n",
        "\n",
        "if method_number == 1:\n",
        "  MEAN_COORDS /= len(features_train_no_split)\n",
        "  print(\"MIN=\",MIN_COORDS)\n",
        "  print(\"MAX=\",MAX_COORDS)\n",
        "  print(\"MEAN=\",MEAN_COORDS)\n",
        "elif method_number == 2:\n",
        "  all_coords = np.concatenate(all_coords)\n",
        "  all_features = np.concatenate(all_features)\n",
        "  all_dist = np.concatenate(all_dist)\n",
        "  transformer.fit(all_coords)\n",
        "  transformer_amino.fit(all_features)\n",
        "  transformer_dist.fit(all_dist.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGQvP3dwSnep"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.hist(all_dist, density=True, bins=30)  # density=False would make counts\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Data');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsHnAMZbUK02"
      },
      "outputs": [],
      "source": [
        "plt.hist(all_coords[:,0], density=True, bins=30)  # density=False would make counts\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Data');\n",
        "plt.show()\n",
        "\n",
        "plt.hist(all_coords[:,1], density=True, bins=30)  # density=False would make counts\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Data');\n",
        "plt.show()\n",
        "\n",
        "plt.hist(all_coords[:,2], density=True, bins=30)  # density=False would make counts\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Data');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8i_K2NZiIGd"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=1, min_delta=1, file_name=\"model.pt\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "        self.file_name = file_name\n",
        "\n",
        "    def early_stop(self, model, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "            print(f\"Saving best model at: '{self.file_name}'\")\n",
        "            torch.save(model.state_dict(), self.file_name)\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def normalize_features(feat,eps=1e-7):\n",
        "  res = np.copy(feat)\n",
        "  ##########\n",
        "  #Method 1\n",
        "  # res[:,0:3] -= MIN_COORDS\n",
        "  # res[:,0:3] /= (MAX_COORDS-MIN_COORDS)\n",
        "  # Method 2\n",
        "  # res[:,0:3] -= MEAN_COORDS\n",
        "  #Method 3\n",
        "  # coords = res[:,0:3]\n",
        "  # tmp_min,tmp_max = np.min(coords,axis=0),np.max(coords,axis=0)\n",
        "  # res[:,0:3] -= tmp_min\n",
        "  # res[:,0:3] /= (tmp_max-tmp_min + eps)\n",
        "  #Method 4\n",
        "  # coords = res[:,0:3]\n",
        "  # tmp_mean = np.mean(coords,axis=0)\n",
        "  # res[:,0:3] -= tmp_mean\n",
        "  #Method 5\n",
        "  res[:,0:3] = transformer.transform(np.clip(res[:,0:3],-250,250))\n",
        "  res[:,25:] = transformer_amino.transform(res[:,25:])\n",
        "  #################\n",
        "  return res\n",
        "\n",
        "def normalize_edges(efeat):\n",
        "  res = np.copy(efeat)\n",
        "  res[:,0] = transformer_dist.transform(np.clip(res[:,0],0,10).reshape(-1,1)).flatten()\n",
        "  return res\n",
        "\n",
        "def separate_edges(edge_index,edge_features,n=2):\n",
        "  # print(edge_index)\n",
        "  if n == 2:\n",
        "    idx_bond = np.where(np.logical_or(edge_features[:,2],edge_features[:,4]))[0] #peptide or hydrogen\n",
        "    idx_distance = np.where(np.logical_or(edge_features[:,1],edge_features[:,3]))[0] #distance\n",
        "\n",
        "    return edge_index[:,idx_bond],edge_index[:,idx_distance]\n",
        "  elif n== 4:\n",
        "    idx_1 = np.where(edge_features[:,0])[0]\n",
        "    idx_2 = np.where(edge_features[:,1])[0]\n",
        "    idx_3 = np.where(edge_features[:,2])[0]\n",
        "    idx_4 = np.where(edge_features[:,3])[0]\n",
        "\n",
        "    return edge_index[:,idx_1],edge_index[:,idx_2],edge_index[:,idx_3],edge_index[:,idx_4]\n",
        "  else:\n",
        "    raise ValueError(\"Not implemented\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOx0Yvtb2i3T"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "adj_train, adj_val, \\\n",
        "features_train, features_val, \\\n",
        "edges_features_train, edges_features_val,\\\n",
        "y_train, y_val = train_test_split(adj_train_no_split,features_train_no_split,\n",
        "                                  edges_features_train_no_split, y_train_no_split, \n",
        "                                  test_size=0.2, random_state=42, stratify=y_train_no_split)\n",
        "N_train = len(features_train)\n",
        "N_val = len(features_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1OH0E6acXWE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUxEUyXjRtbO"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epochs = 200\n",
        "batch_size = 64\n",
        "n_input = 86\n",
        "dropout = 0.5\n",
        "learning_rate = 1e-3\n",
        "n_class = 18\n",
        "##########################\n",
        "# Tunable\n",
        "n_hidden = 64\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "NEW_NET = False\n",
        "GRAPHS2 = True\n",
        "GRAPHS4 = False\n",
        "USE_EDGE_FEATURES = True\n",
        "\n",
        "\n",
        "# model = GIN(n_input, n_hidden, dropout, n_class).to(device)\n",
        "# with edge attributes\n",
        "model = GIN2Graphs(n_input, n_hidden, dropout, n_class).to(device)\n",
        "\n",
        "# model = GIN4Graphs(n_input, n_hidden, dropout, n_class).to(device)\n",
        "# model = GINE(n_input, n_hidden, dropout, n_class).to(device)\n",
        "# model = PDN(n_input, n_hidden, dropout, n_class).to(device)\n",
        "\n",
        "#optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss(reduction='mean',label_smoothing=0.0)\n",
        "model_name = f\"2Graphs_nhidden{n_hidden}_normalized.pt\"\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.1, file_name=model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1pBBWPCi9uy"
      },
      "outputs": [],
      "source": [
        "def validation(model,normalize=False):\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, N_val, batch_size):\n",
        "          adj_batch = list()\n",
        "          features_batch = list()\n",
        "          if USE_EDGE_FEATURES:\n",
        "            edges_features_batch = list()\n",
        "          idx_batch = list()\n",
        "          y_batch = list()\n",
        "          \n",
        "          # Create tensors\n",
        "          for j in range(i, min(N_val, i+batch_size)):\n",
        "              n = adj_val[j].shape[0]\n",
        "              adj_batch.append(adj_val[j]+sp.identity(n))\n",
        "              ft_bt = features_val[j]\n",
        "              if normalize:\n",
        "                ft_bt = normalize_features(ft_bt)\n",
        "              features_batch.append(ft_bt)\n",
        "              if USE_EDGE_FEATURES:\n",
        "                ed_ft = edges_features_val[j]\n",
        "                if normalize:\n",
        "                  ed_ft = normalize_edges(ed_ft)\n",
        "                edges_features_batch.append(ed_ft)\n",
        "              idx_batch.extend([j-i]*n)\n",
        "              y_batch.append(y_val[j])\n",
        "              \n",
        "          adj_batch = sp.block_diag(adj_batch)\n",
        "          features_batch = np.vstack(features_batch)\n",
        "          if USE_EDGE_FEATURES:\n",
        "            edges_features_batch = np.vstack(edges_features_batch)\n",
        "          adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device).to_dense()\n",
        "          # From https://discuss.pytorch.org/t/adjacency-matrix-to-edge-index-solution/148343\n",
        "          # To get the edge_index representation from the adj matrix\n",
        "          edge_index = adj_batch.nonzero().t().contiguous()\n",
        "\n",
        "          if GRAPHS2:\n",
        "            edge_index_bond, edge_index_distance = separate_edges(edge_index,edges_features_batch)\n",
        "          elif GRAPHS4:\n",
        "            e1,e2,e3,e4 = separate_edges(edge_index,edges_features_batch,n=4)\n",
        "          \n",
        "          if USE_EDGE_FEATURES and not (GRAPHS2 or GRAPHS4):\n",
        "            edges_features_batch = torch.FloatTensor(edges_features_batch).to(device)\n",
        "          features_batch = torch.FloatTensor(features_batch).to(device)\n",
        "          idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "          y_batch = torch.LongTensor(y_batch).to(device)\n",
        "\n",
        "          if GRAPHS2:\n",
        "            _, output = model(features_batch, edge_index_distance,edge_index_bond, idx_batch)\n",
        "          elif GRAPHS4:\n",
        "            _, output = model(features_batch,e1,e2,e3,e4, idx_batch)\n",
        "          elif USE_EDGE_FEATURES:\n",
        "            _, output = model(features_batch, edge_index, edges_features_batch, idx_batch)\n",
        "          elif NEW_NET:\n",
        "            output = model(features_batch, edge_index, idx_batch)\n",
        "          else:\n",
        "            _,output = model(features_batch, edge_index, idx_batch)\n",
        "          loss += loss_function(output, y_batch).item()*output.size(0)\n",
        "          count += output.size(0)\n",
        "          preds = output.max(1)[1].type_as(y_batch)\n",
        "          correct += torch.sum(preds.eq(y_batch).double())\n",
        "  return loss/count,correct/count\n",
        "        \n",
        "      \n",
        "        \n",
        "\n",
        "def train(model,normalize=False):\n",
        "  # Train model\n",
        "  print(f\"Training Model {model_name}\")\n",
        "  for epoch in range(epochs):\n",
        "      t = time.time()\n",
        "      train_loss = 0\n",
        "      correct = 0\n",
        "      count = 0\n",
        "      # Iterate over the batches\n",
        "      for i in range(0, N_train, batch_size):\n",
        "          model.train()\n",
        "          adj_batch = list()\n",
        "          features_batch = list()\n",
        "          if USE_EDGE_FEATURES:\n",
        "            edges_features_batch = list()\n",
        "          idx_batch = list()\n",
        "          y_batch = list()\n",
        "          \n",
        "          # Create tensors\n",
        "\n",
        "          for j in range(i, min(N_train, i+batch_size)):\n",
        "              n = adj_train[j].shape[0]\n",
        "              adj_batch.append(adj_train[j]+sp.identity(n))\n",
        "              ft_bt = features_train[j]\n",
        "              if normalize:\n",
        "                ft_bt = normalize_features(ft_bt)\n",
        "              features_batch.append(ft_bt)\n",
        "              if USE_EDGE_FEATURES:\n",
        "                ed_ft = edges_features_train[j]\n",
        "                if normalize:\n",
        "                  ed_ft = normalize_edges(ed_ft)\n",
        "                edges_features_batch.append(ed_ft)\n",
        "              idx_batch.extend([j-i]*n)\n",
        "              y_batch.append(y_train[j])\n",
        "              \n",
        "          adj_batch = sp.block_diag(adj_batch)\n",
        "          features_batch = np.vstack(features_batch)\n",
        "          if USE_EDGE_FEATURES:\n",
        "            edges_features_batch = np.vstack(edges_features_batch)\n",
        "          adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device).to_dense()\n",
        "          # From https://discuss.pytorch.org/t/adjacency-matrix-to-edge-index-solution/148343\n",
        "          # To get the edge_index representation from the adj matrix\n",
        "          edge_index = adj_batch.nonzero().t().contiguous()\n",
        "\n",
        "          if GRAPHS2:\n",
        "            edge_index_bond, edge_index_distance = separate_edges(edge_index,edges_features_batch)\n",
        "          elif GRAPHS4:\n",
        "            e1,e2,e3,e4 = separate_edges(edge_index,edges_features_batch,n=4)\n",
        "\n",
        "          if USE_EDGE_FEATURES and not (GRAPHS2 or GRAPHS4):\n",
        "              edges_features_batch = torch.FloatTensor(edges_features_batch).to(device)\n",
        "          # edge_index = torch.LongTensor(edge_index).to(device)\n",
        "          features_batch = torch.FloatTensor(features_batch).to(device)\n",
        "          idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "          y_batch = torch.LongTensor(y_batch).to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          if GRAPHS2:\n",
        "            _, output = model(features_batch, edge_index_distance,edge_index_bond, idx_batch)\n",
        "          elif GRAPHS4:\n",
        "            _, output = model(features_batch, e1,e2,e3,e4, idx_batch)\n",
        "          elif USE_EDGE_FEATURES:\n",
        "            _, output = model(features_batch, edge_index,edges_features_batch, idx_batch)\n",
        "          elif NEW_NET:\n",
        "            output = model(features_batch, edge_index, idx_batch)\n",
        "          else:\n",
        "            _,output = model(features_batch, edge_index, idx_batch)\n",
        "          loss = loss_function(output, y_batch)\n",
        "          train_loss += loss.item() * output.size(0)\n",
        "          count += output.size(0)\n",
        "          preds = output.max(1)[1].type_as(y_batch)\n",
        "          correct += torch.sum(preds.eq(y_batch).double())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      \n",
        "      if False:\n",
        "          print('Epoch: {:03d}'.format(epoch+1),\n",
        "                'loss_train: {:.4f}'.format(train_loss / count),\n",
        "                'acc_train: {:.4f}'.format(correct / count),\n",
        "                'time: {:.4f}s'.format(time.time() - t))\n",
        "      if epoch % 1 == 0:\n",
        "        loss_val,acc_val = validation(model,normalize=normalize)\n",
        "        print('Epoch: {:03d}'.format(epoch+1),\n",
        "                'loss_train: {:.4f}'.format(train_loss / count),\n",
        "                'acc_train: {:.4f}'.format(correct / count),\n",
        "                'loss_val: {:.4f}'.format(loss_val),\n",
        "                'acc_val: {:.4f}'.format(acc_val),\n",
        "                'time: {:.4f}s'.format(time.time() - t))\n",
        "        if early_stopping.early_stop(model, loss_val):\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,normalize=True)"
      ],
      "metadata": {
        "id": "2rA8AHu94F8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "print(\"Validation of model:\",validation(model,normalize=True))"
      ],
      "metadata": {
        "id": "2ZsY3blM4RiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBhRwE3gyBSY"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate/10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "train(model,normalize=True)"
      ],
      "metadata": {
        "id": "Sf8Yoxuc4Sbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "print(\"Validation of model:\",validation(model,normalize=True))"
      ],
      "metadata": {
        "id": "uZd9IVkD5tzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbo_yKTWcbJ1"
      },
      "source": [
        "# Creating Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SerklifrjBVf"
      },
      "outputs": [],
      "source": [
        "N_test = len(adj_test)\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "y_pred_proba = list()\n",
        "# Iterate over the batches\n",
        "for i in range(0, N_test, batch_size):\n",
        "    adj_batch = list()\n",
        "    idx_batch = list()\n",
        "    features_batch = list()\n",
        "    y_batch = list()\n",
        "    edges_features_batch = list()\n",
        "    \n",
        "    # Create tensors\n",
        "    for j in range(i, min(N_test, i+batch_size)):\n",
        "        n = adj_test[j].shape[0]\n",
        "        adj_batch.append(adj_test[j]+sp.identity(n))\n",
        "        features_batch.append(normalize_features(features_test[j]))\n",
        "        idx_batch.extend([j-i]*n)\n",
        "        if USE_EDGE_FEATURES:\n",
        "          edges_features_batch.append(normalize_edges(edges_features_test[j]))\n",
        "            \n",
        "    if USE_EDGE_FEATURES and not GRAPHS2:\n",
        "        edges_features_batch = torch.FloatTensor(edges_features_batch).to(device)\n",
        "    adj_batch = sp.block_diag(adj_batch)\n",
        "    features_batch = np.vstack(features_batch)\n",
        "    if USE_EDGE_FEATURES:\n",
        "      edges_features_batch = np.vstack(edges_features_batch)\n",
        "    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device).to_dense()\n",
        "    # From https://discuss.pytorch.org/t/adjacency-matrix-to-edge-index-solution/148343\n",
        "    # To get the edge_index representation from the adj matrix\n",
        "    edge_index = adj_batch.nonzero().t().contiguous()\n",
        "    if GRAPHS2:\n",
        "      edge_index_bond, edge_index_distance = separate_edges(edge_index,edges_features_batch)\n",
        "\n",
        "    features_batch = torch.FloatTensor(features_batch).to(device)\n",
        "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "    if GRAPHS2:\n",
        "      _,output = model(features_batch, edge_index_distance,edge_index_bond, idx_batch)\n",
        "    else:\n",
        "      _,output = model(features_batch, edge_index, idx_batch)\n",
        "    y_pred_proba.append(output)\n",
        "    \n",
        "y_pred_proba = torch.cat(y_pred_proba, dim=0)\n",
        "y_pred_proba = torch.exp(y_pred_proba)\n",
        "y_pred_proba = y_pred_proba.detach().cpu().numpy()\n",
        "\n",
        "# Write predictions to a file\n",
        "with open('sample_submission.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(18):\n",
        "        lst.append('class'+str(i))\n",
        "    lst.insert(0, \"name\")\n",
        "    writer.writerow(lst)\n",
        "    for i, protein in enumerate(proteins_test):\n",
        "        lst = y_pred_proba[i,:].tolist()\n",
        "        lst.insert(0, protein)\n",
        "        writer.writerow(lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LUUcsKoAhU-"
      },
      "source": [
        " # Trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNKWa12psyBH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# else:\n",
        "#   edges, features, edge_features = load_data(new_edge_loading=NEW_EDGE_LOADING) \n",
        "\n",
        "#   # Split data into training and test sets\n",
        "#   edges_train_no_split = list()\n",
        "#   features_train_no_split = list()\n",
        "#   edges_features_train_no_split = list()\n",
        "#   y_train_no_split = list()\n",
        "#   edges_test = list()\n",
        "#   features_test = list()\n",
        "#   edges_features_test = list()\n",
        "#   proteins_test = list()\n",
        "#   print(\"Loading Data\")\n",
        "#   with open(os.path.join(PATH,'graph_labels.txt'), 'r') as f:\n",
        "#       for i,line in tqdm(enumerate(f)):\n",
        "#           t = line.split(',')\n",
        "#           if len(t[1][:-1]) == 0:\n",
        "#               proteins_test.append(t[0])\n",
        "#               edges_test.append(edges[i])\n",
        "#               features_test.append(features[i])\n",
        "#               edges_features_test.append(edge_features[i])\n",
        "#           else:\n",
        "#               edges_train_no_split.append(edges[i])\n",
        "#               features_train_no_split.append(features[i])\n",
        "#               edges_features_train_no_split.append(edge_features[i])\n",
        "#               y_train_no_split.append(int(t[1][:-1]))\n",
        "\n",
        "# # Initialize device\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# # Compute number of training and test samples\n",
        "# N_train_no_split = len(features_train_no_split)\n",
        "# N_test = len(features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiULYNc-SZS1"
      },
      "outputs": [],
      "source": [
        "# #split data\n",
        "# split_percentage = 0.9\n",
        "# N = N_train_no_split\n",
        "# indexes = np.arange(N)\n",
        "# np.random.shuffle(indexes)\n",
        "# idx_train = indexes[:int(N*split_percentage)]\n",
        "# idx_val = indexes[int(N*split_percentage):]\n",
        "\n",
        "# adj_train = list()\n",
        "# features_train = list()\n",
        "# edges_features_train = list()\n",
        "# y_train = list()\n",
        "\n",
        "# adj_val = list()\n",
        "# features_val = list()\n",
        "# edges_features_val = list()\n",
        "# y_val = list()\n",
        "\n",
        "# for i in range(N):\n",
        "#   if i in idx_train:\n",
        "#     adj_train.append(adj_train_no_split[i])\n",
        "#     features_train.append(features_train_no_split[i])\n",
        "#     edges_features_train.append(edges_features_train_no_split[i])\n",
        "#     y_train.append(y_train_no_split[i])\n",
        "#   elif i in idx_val:\n",
        "#     adj_val.append(adj_train_no_split[i])\n",
        "#     features_val.append(features_train_no_split[i])\n",
        "#     edges_features_val.append(edges_features_train_no_split[i])\n",
        "#     y_val.append(y_train_no_split[i])\n",
        "#   else:\n",
        "#     raise ValueError(f\"Index i={i} is not in the training or validation set.\")\n",
        "\n",
        "\n",
        "# N_train = len(adj_train)\n",
        "# N_val = len(adj_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAkxxZyG9PWF"
      },
      "outputs": [],
      "source": [
        "# graph_indicator = np.loadtxt(os.path.join(PATH,\"graph_indicator.txt\"), dtype=np.int64)\n",
        "# _,graph_size = np.unique(graph_indicator, return_counts=True)\n",
        "\n",
        "# edges = np.loadtxt(os.path.join(PATH,\"edgelist.txt\"), dtype=np.int64, delimiter=\",\")\n",
        "# edges_inv = np.vstack((edges[:,1], edges[:,0]))\n",
        "# edges = np.vstack((edges, edges_inv.T))\n",
        "# s = edges[:,0]*graph_indicator.size + edges[:,1]\n",
        "# idx_sort = np.argsort(s)\n",
        "# edges = edges[idx_sort,:]\n",
        "# edges,idx_unique =  np.unique(edges, axis=0, return_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmKODEZzAfPw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# #TODO: Edge Type try RGATConv, FiLMConv,RGCNConv\n",
        "# #TODO: FeaturesTry NNConv,GENConv,\n",
        "# def load_data(): \n",
        "#     \"\"\"\n",
        "#     Function that loads graphs\n",
        "#     \"\"\"  \n",
        "#     graph_indicator = np.loadtxt(os.path.join(PATH,\"graph_indicator.txt\"), dtype=np.int64)\n",
        "#     _,graph_size = np.unique(graph_indicator, return_counts=True)\n",
        "    \n",
        "#     edges = np.loadtxt(os.path.join(PATH,\"edgelist.txt\"), dtype=np.int64, delimiter=\",\")\n",
        "#     A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n",
        "#     A += A.T\n",
        "    \n",
        "#     x = np.loadtxt(os.path.join(PATH,\"node_attributes.txt\"), delimiter=\",\")\n",
        "#     edge_attr = np.loadtxt(os.path.join(PATH,\"edge_attributes.txt\"), delimiter=\",\")\n",
        "    \n",
        "#     adj = []\n",
        "#     features = []\n",
        "#     edge_features = []\n",
        "#     idx_n = 0\n",
        "#     idx_m = 0\n",
        "#     for i in tqdm(range(graph_size.size)):\n",
        "#         n_edges = adj[i].nnz\n",
        "#         adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n",
        "#         edge_features.append(edge_attr[idx_m:idx_m+n_edges,:])\n",
        "#         features.append(x[idx_n:idx_n+graph_size[i],:])\n",
        "#         idx_n += graph_size[i]\n",
        "#         idx_m += n_edges\n",
        "\n",
        "#     return adj, features, edge_features\n",
        "\n",
        "# def normalize_adjacency(A):\n",
        "#     \"\"\"\n",
        "#     Function that normalizes an adjacency matrix\n",
        "#     \"\"\"\n",
        "#     n = A.shape[0]\n",
        "#     A = A + sp.identity(n)\n",
        "#     degs = A.dot(np.ones(n))\n",
        "#     inv_degs = np.power(degs, -1)\n",
        "#     D = sp.diags(inv_degs)\n",
        "#     A_normalized = D.dot(A)\n",
        "\n",
        "#     return A_normalized\n",
        "\n",
        "# def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "#     \"\"\"\n",
        "#     Function that converts a Scipy sparse matrix to a sparse Torch tensor\n",
        "#     \"\"\"\n",
        "#     sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "#     indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "#     values = torch.from_numpy(sparse_mx.data)\n",
        "#     shape = torch.Size(sparse_mx.shape)\n",
        "#     return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ShsCTxFrkcB"
      },
      "outputs": [],
      "source": [
        "# if new_edge_loading:\n",
        "    #   edges = edges.T\n",
        "    # if new_edge_loading:\n",
        "    #   adj = []\n",
        "    #   edges_index = []\n",
        "    #   features = []\n",
        "    #   edge_features = []\n",
        "    #   idx_n = 0\n",
        "    #   idx_m = 0\n",
        "    #   for i in range(graph_size.size):\n",
        "    #       adjecency_matrix = A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]]\n",
        "    #       adj.append(adjecency_matrix)\n",
        "    #       #fixing edge representatoin\n",
        "    #       # e = edges[:,idx_m:idx_m+adj[i].nnz]\n",
        "    #       # e -= np.min(e)\n",
        "    #       edge_index,_ = from_scipy_sparse_matrix(adjecency_matrix)\n",
        "          \n",
        "    #       edges_index.append(edge_index)\n",
        "    #       # assert np.min(edges_index[-1]) == 0 \n",
        "    #       feat = edge_attr[idx_m:idx_m+adj[i].nnz,:]\n",
        "    #       edge_features.append(feat)\n",
        "    #       ##########\n",
        "          \n",
        "    #       features.append(x[idx_n:idx_n+graph_size[i],:])\n",
        "    #       idx_n += graph_size[i]\n",
        "    #       idx_m += adj[i].nnz\n",
        "\n",
        "    #   return edges_index, features, edge_features\n",
        "\n",
        "    # else:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwFMoMMex8zH"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(features_val)):\n",
        "#   features_val[i] = normalize_features(features_val[i])\n",
        "#   edges_features_val[i] = normalize_edges(edges_features_val[i])\n",
        "\n",
        "# for i in range(len(features_train)):\n",
        "#   features_train[i] = normalize_features(features_train[i])\n",
        "#   edges_features_train[i] = normalize_edges(edges_features_train[i])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cdq_zhclcKEY",
        "uDkMucYsHNQ3",
        "rG-X20nVcPAJ",
        "dbo_yKTWcbJ1",
        "8LUUcsKoAhU-"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}